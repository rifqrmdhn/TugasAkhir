{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rifqrmdhn/TugasAkhir/blob/main/Bismillah_TA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNgU3AR2lhsP",
        "outputId": "b141f02e-5dd7-4de8-e8f0-063370b2b4c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mtcnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFEAKidpvlcx",
        "outputId": "e408672f-9ac0-452f-fdd4-b2fea89d0c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mtcnn\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (2.12.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.23.5)\n",
            "Installing collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mtcnn import MTCNN\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow  # Menggunakan cv2_imshow dari Google Colab\n",
        "\n",
        "# Path ke direktori dataset\n",
        "dataset_dir = '/content/drive/MyDrive/Data TA'\n",
        "\n",
        "# Fungsi untuk ekstraksi wajah menggunakan MTCNN\n",
        "def extract_face(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    detector = MTCNN()\n",
        "    faces = detector.detect_faces(image)\n",
        "\n",
        "    if len(faces) > 0:\n",
        "        x, y, w, h = faces[0]['box']\n",
        "        face = image[y:y+h, x:x+w]\n",
        "        return face\n",
        "    else:\n",
        "        return None\n",
        "# Fungsi untuk membaca dan melakukan preprocessing pada gambar wajah\n",
        "def preprocess_image(face_image):\n",
        "    # Ubah gambar wajah ke skala abu-abu\n",
        "    gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Menghapus noise dengan filter median\n",
        "    gray = cv2.medianBlur(gray, ksize=3)\n",
        "\n",
        "    # Normalisasi nilai piksel\n",
        "    normalized = gray / 255.0\n",
        "\n",
        "    return normalized\n",
        "\n",
        "\n",
        "# Fungsi untuk memuat dataset dan mengekstrak wajah\n",
        "def load_and_extract_dataset():\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop melalui setiap direktori kelas dalam dataset\n",
        "    for class_dir in os.listdir(dataset_dir):\n",
        "        if os.path.isdir(os.path.join(dataset_dir, class_dir)):\n",
        "            class_label = class_dir\n",
        "\n",
        "            # Loop melalui setiap gambar dalam direktori kelas\n",
        "            for image_file in os.listdir(os.path.join(dataset_dir, class_dir)):\n",
        "                if image_file.endswith('.png'):\n",
        "                    image_path = os.path.join(dataset_dir, class_dir, image_file)\n",
        "\n",
        "                    # Ekstrak wajah dari gambar\n",
        "                    extracted_face = extract_face(image_path)\n",
        "\n",
        "                    if extracted_face is not None:\n",
        "                        # Preprocess gambar wajah\n",
        "                        preprocessed_image = preprocess_image(extracted_face)\n",
        "\n",
        "                        # Tambahkan gambar dan label ke list\n",
        "                        images.append(preprocessed_image)\n",
        "                        labels.append(class_label)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Memuat dataset dan mengekstrak wajah\n",
        "images, labels = load_and_extract_dataset()\n",
        "\n",
        "# Memeriksa dimensi data\n",
        "print(\"Dimensi images:\", images.shape)\n",
        "print(\"Dimensi labels:\", labels.shape)\n",
        "\n",
        "# Memeriksa jumlah kelas unik\n",
        "unique_labels = np.unique(labels)\n",
        "print(\"Jumlah kelas unik:\", len(unique_labels))\n",
        "print(\"Kelas unik:\", unique_labels)\n",
        "\n",
        "# Memeriksa apakah jumlah data dan label cocok\n",
        "if len(images) != len(labels):\n",
        "    print(\"Jumlah data dan label tidak cocok!\")\n",
        "\n",
        "# Melakukan pembagian dataset menjadi set pelatihan dan pengujian\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Membagi dataset menjadi set pelatihan dan pengujian dengan perbandingan 60:40\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.4, train_size=0.6, random_state=42)\n",
        "\n",
        "# Memeriksa dimensi set pelatihan dan pengujian\n",
        "print(\"Dimensi train_images:\", train_images.shape)\n",
        "print(\"Dimensi test_images:\", test_images.shape)\n",
        "print(\"Dimensi train_labels:\", train_labels.shape)\n",
        "print(\"Dimensi test_labels:\", test_labels.shape)\n",
        "\n",
        "# Memeriksa apakah jumlah data dan label pada set pelatihan dan pengujian cocok\n",
        "if len(train_images) != len(train_labels):\n",
        "    print(\"Jumlah data dan label pada set pelatihan tidak cocok!\")\n",
        "if len(test_images) != len(test_labels):\n",
        "    print(\"Jumlah data dan label pada set pengujian tidak cocok!\")\n",
        "\n",
        "# Ubah dimensi array images sesuai dengan jumlah channel\n",
        "train_images = np.expand_dims(train_images, axis=1)\n",
        "test_images = np.expand_dims(test_images, axis=1)\n",
        "\n",
        "# Membuat objek LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Melakukan transformasi label menjadi angka\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)\n",
        "\n",
        "# Konversi numpy array menjadi PyTorch tensors\n",
        "train_images = torch.tensor(train_images, dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels_encoded, dtype=torch.long)\n",
        "test_images = torch.tensor(test_images, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test_labels_encoded, dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZWM42E8HvsHx",
        "outputId": "90264f93-48ad-4b3b-d809-f9574661fb1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 221ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 206ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 224ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 236ms/step\n",
            "1/1 [==============================] - 0s 211ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 1s 593ms/step\n",
            "1/1 [==============================] - 0s 451ms/step\n",
            "1/1 [==============================] - 0s 422ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 328ms/step\n",
            "1/1 [==============================] - 0s 499ms/step\n",
            "1/1 [==============================] - 0s 290ms/step\n",
            "1/1 [==============================] - 0s 366ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 271ms/step\n",
            "1/1 [==============================] - 0s 278ms/step\n",
            "1/1 [==============================] - 0s 248ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 368ms/step\n",
            "1/1 [==============================] - 0s 295ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 245ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 399ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 261ms/step\n",
            "1/1 [==============================] - 1s 607ms/step\n",
            "1/1 [==============================] - 0s 495ms/step\n",
            "1/1 [==============================] - 1s 602ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 367ms/step\n",
            "1/1 [==============================] - 0s 309ms/step\n",
            "1/1 [==============================] - 0s 223ms/step\n",
            "1/1 [==============================] - 0s 281ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 227ms/step\n",
            "1/1 [==============================] - 0s 242ms/step\n",
            "1/1 [==============================] - 0s 294ms/step\n",
            "1/1 [==============================] - 0s 253ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 226ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 217ms/step\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "1/1 [==============================] - 0s 201ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 276ms/step\n",
            "1/1 [==============================] - 0s 338ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 249ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 1s 918ms/step\n",
            "1/1 [==============================] - 0s 219ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 245ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 240ms/step\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 219ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "1/1 [==============================] - 0s 226ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 1s 901ms/step\n",
            "1/1 [==============================] - 0s 232ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 226ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "1/1 [==============================] - 0s 237ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 236ms/step\n",
            "1/1 [==============================] - 0s 198ms/step\n",
            "1/1 [==============================] - 0s 209ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 252ms/step\n",
            "1/1 [==============================] - 0s 306ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 247ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 299ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 348ms/step\n",
            "1/1 [==============================] - 0s 476ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 220ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 311ms/step\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "1/1 [==============================] - 0s 203ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 248ms/step\n",
            "1/1 [==============================] - 0s 266ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 214ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 228ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 226ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 265ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "1/1 [==============================] - 0s 230ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 201ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 237ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 197ms/step\n",
            "1/1 [==============================] - 0s 216ms/step\n",
            "1/1 [==============================] - 1s 981ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 189ms/step\n",
            "1/1 [==============================] - 0s 230ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 193ms/step\n",
            "1/1 [==============================] - 0s 228ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 246ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 189ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "1/1 [==============================] - 0s 234ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 219ms/step\n",
            "1/1 [==============================] - 0s 198ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 252ms/step\n",
            "1/1 [==============================] - 0s 283ms/step\n",
            "1/1 [==============================] - 0s 217ms/step\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 278ms/step\n",
            "1/1 [==============================] - 0s 326ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 225ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 225ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 225ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 256ms/step\n",
            "1/1 [==============================] - 0s 362ms/step\n",
            "1/1 [==============================] - 0s 207ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 292ms/step\n",
            "1/1 [==============================] - 0s 278ms/step\n",
            "1/1 [==============================] - 0s 222ms/step\n",
            "1/1 [==============================] - 0s 232ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 275ms/step\n",
            "1/1 [==============================] - 0s 243ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 238ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 189ms/step\n",
            "1/1 [==============================] - 0s 229ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 207ms/step\n",
            "1/1 [==============================] - 0s 245ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 244ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 251ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 198ms/step\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 267ms/step\n",
            "1/1 [==============================] - 0s 314ms/step\n",
            "1/1 [==============================] - 0s 210ms/step\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 293ms/step\n",
            "1/1 [==============================] - 0s 305ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 243ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "1/1 [==============================] - 0s 233ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 1s 876ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 224ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 246ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 198ms/step\n",
            "1/1 [==============================] - 0s 278ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 193ms/step\n",
            "1/1 [==============================] - 0s 242ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 235ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 239ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 221ms/step\n",
            "1/1 [==============================] - 0s 226ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "1/1 [==============================] - 0s 221ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 227ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 253ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 250ms/step\n",
            "1/1 [==============================] - 0s 313ms/step\n",
            "1/1 [==============================] - 0s 223ms/step\n",
            "1/1 [==============================] - 0s 201ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 232ms/step\n",
            "1/1 [==============================] - 0s 335ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 225ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 229ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 192ms/step\n",
            "1/1 [==============================] - 0s 226ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "1/1 [==============================] - 0s 232ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 220ms/step\n",
            "1/1 [==============================] - 0s 262ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 223ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 229ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 1s 971ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 222ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "1/1 [==============================] - 0s 224ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 223ms/step\n",
            "1/1 [==============================] - 0s 237ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 229ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 278ms/step\n",
            "1/1 [==============================] - 0s 319ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 258ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 219ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 232ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 229ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 198ms/step\n",
            "1/1 [==============================] - 0s 221ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 254ms/step\n",
            "1/1 [==============================] - 0s 325ms/step\n",
            "1/1 [==============================] - 0s 217ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "1/1 [==============================] - 0s 277ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 189ms/step\n",
            "1/1 [==============================] - 0s 223ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 211ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 236ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 189ms/step\n",
            "1/1 [==============================] - 0s 232ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 257ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "1/1 [==============================] - 0s 222ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 202ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 219ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 216ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 244ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 200ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 226ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 274ms/step\n",
            "1/1 [==============================] - 0s 323ms/step\n",
            "1/1 [==============================] - 0s 197ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 277ms/step\n",
            "1/1 [==============================] - 0s 321ms/step\n",
            "1/1 [==============================] - 0s 206ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 216ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 192ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 245ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 250ms/step\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 220ms/step\n",
            "1/1 [==============================] - 0s 244ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 271ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "1/1 [==============================] - 0s 214ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 178ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 232ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 189ms/step\n",
            "1/1 [==============================] - 0s 241ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 203ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 202ms/step\n",
            "1/1 [==============================] - 0s 227ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 243ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "Dimensi images: (326,)\n",
            "Dimensi labels: (326,)\n",
            "Jumlah kelas unik: 8\n",
            "Kelas unik: ['Bagas' 'Didit' 'Mari' 'Rifqi' 'Romi' 'ikhsanudin' 'momo' 'sendy']\n",
            "Dimensi train_images: (195,)\n",
            "Dimensi test_images: (131,)\n",
            "Dimensi train_labels: (195,)\n",
            "Dimensi test_labels: (131,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-a9d2deb2ade3>:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(images), np.array(labels)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a9d2deb2ade3>\u001b[0m in \u001b[0;36m<cell line: 113>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# Konversi numpy array menjadi PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YN8vToO559P"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "# Path ke direktori dataset\n",
        "dataset_dir = '/content/drive/MyDrive/Data TA'\n",
        "\n",
        "# Fungsi untuk membaca dan melakukan preprocessing pada gambar wajah\n",
        "def preprocess_image(image_path):\n",
        "    # Baca gambar wajah menggunakan OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Ubah ke skala abu-abu\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Menghapus noise dengan filter median\n",
        "    gray = cv2.medianBlur(gray, ksize=3)\n",
        "\n",
        "    # Normalisasi nilai piksel\n",
        "    normalized = gray / 255.0\n",
        "\n",
        "    return normalized\n",
        "\n",
        "# Fungsi untuk memuat dataset\n",
        "def load_dataset():\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop melalui setiap direktori kelas dalam dataset\n",
        "    for class_dir in os.listdir(dataset_dir):\n",
        "        if os.path.isdir(os.path.join(dataset_dir, class_dir)):\n",
        "            class_label = class_dir\n",
        "\n",
        "            # Loop melalui setiap gambar dalam direktori kelas\n",
        "            for image_file in os.listdir(os.path.join(dataset_dir, class_dir)):\n",
        "                if image_file.endswith('.png'):\n",
        "                    image_path = os.path.join(dataset_dir, class_dir, image_file)\n",
        "\n",
        "                    # Preprocess gambar wajah\n",
        "                    preprocessed_image = preprocess_image(image_path)\n",
        "\n",
        "                    # Tambahkan gambar dan label ke list\n",
        "                    images.append(preprocessed_image)\n",
        "                    labels.append(class_label)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Memuat dataset\n",
        "images, labels = load_dataset()\n",
        "\n",
        "# Melakukan pembagian dataset menjadi set pelatihan dan pengujian\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Membagi dataset menjadi set pelatihan dan pengujian dengan perbandingan 60:40\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.4, train_size=0.6, random_state=42)\n",
        "\n",
        "# Melakukan one-hot encoding pada label\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Membuat objek LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Melakukan transformasi label menjadi angka\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)\n",
        "\n",
        "# Konversi numpy array menjadi PyTorch tensors\n",
        "train_images = torch.from_numpy(train_images).unsqueeze(1).float()\n",
        "train_labels = torch.from_numpy(train_labels_encoded).long()\n",
        "test_images = torch.from_numpy(test_images).unsqueeze(1).float()\n",
        "test_labels = torch.from_numpy(test_labels_encoded).long()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1IG4bL_7dg4"
      },
      "source": [
        "# **ResNeXt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpmPmm92jmJl",
        "outputId": "df314beb-5310-4627-8783-51ed6e2ec6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNeXt(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): ResNeXtBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResNeXtBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): ResNeXtBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): ResNeXtBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResNeXtBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): ResNeXtBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): ResNeXtBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): ResNeXtBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResNeXtBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): ResNeXtBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): ResNeXtBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (4): ResNeXtBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (5): ResNeXtBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): ResNeXtBlock(\n",
            "      (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResNeXtBlock(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): ResNeXtBlock(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=1024, out_features=8, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResNeXtBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, cardinality=32):\n",
        "        super(ResNeXtBlock, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.cardinality = cardinality\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = F.relu(self.bn1(out))\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(self.bn2(out))\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        out += self.shortcut(residual)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNeXt(nn.Module):\n",
        "    def __init__(self, num_classes, cardinality=32):\n",
        "        super(ResNeXt, self).__init__()\n",
        "        self.cardinality = cardinality\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self.make_layer(64, 128, 3, stride=1)\n",
        "        self.layer2 = self.make_layer(128, 256, 4, stride=2)\n",
        "        self.layer3 = self.make_layer(256, 512, 6, stride=2)\n",
        "        self.layer4 = self.make_layer(512, 1024, 3, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResNeXtBlock(in_channels, out_channels, stride=stride, cardinality=self.cardinality))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(ResNeXtBlock(out_channels, out_channels, stride=1, cardinality=self.cardinality))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "input_shape = (1, 224, 224)  # Input shape of your grayscale images in PyTorch convention (channels, height, width)\n",
        "num_classes = 8 # Number of output classes\n",
        "model = ResNeXt(num_classes)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5DVqYKHvmiC",
        "outputId": "f750e142-cae8-446f-9b2e-af70944fc8f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss = 2.147212877869606\n",
            "Accuracy: 0.25416666666666665, F1 Score: 0.25509454583749563, Precision: 0.292748673998674, Recall: 0.24994962564075304\n",
            "Epoch 2: Loss = 0.844360738992691\n",
            "Accuracy: 0.7333333333333333, F1 Score: 0.731342822291343, Precision: 0.7442324236561986, Recall: 0.7295909116559512\n",
            "Epoch 3: Loss = 0.49347864277660847\n",
            "Accuracy: 0.8458333333333333, F1 Score: 0.8433379701572442, Precision: 0.846745107217332, Recall: 0.8431339718840536\n",
            "Epoch 4: Loss = 0.3103577010333538\n",
            "Accuracy: 0.9083333333333333, F1 Score: 0.9110804198787756, Precision: 0.9147536971777959, Recall: 0.9102254829022316\n",
            "Epoch 5: Loss = 0.15068289916962385\n",
            "Accuracy: 0.9541666666666667, F1 Score: 0.9546424403880588, Precision: 0.9560655623155624, Recall: 0.9540035766050783\n",
            "Epoch 6: Loss = 0.08073304011486471\n",
            "Accuracy: 0.975, F1 Score: 0.9757591369811162, Precision: 0.9756552419354838, Recall: 0.9759834159166751\n",
            "Epoch 7: Loss = 0.054240144265349954\n",
            "Accuracy: 0.9875, F1 Score: 0.9874965268130036, Precision: 0.98828125, Recall: 0.9879032258064516\n",
            "Epoch 8: Loss = 0.0888675760361366\n",
            "Accuracy: 0.9791666666666666, F1 Score: 0.9786937344382198, Precision: 0.9808170995670996, Recall: 0.9782943349753694\n",
            "Epoch 9: Loss = 0.034241279878187925\n",
            "Accuracy: 0.9875, F1 Score: 0.9872023809523809, Precision: 0.987471198156682, Recall: 0.987471198156682\n",
            "Epoch 10: Loss = 0.08477545354980975\n",
            "Accuracy: 0.975, F1 Score: 0.9735890803068994, Precision: 0.9742236929736929, Recall: 0.9733454205436964\n",
            "Epoch 11: Loss = 0.03249634837266058\n",
            "Accuracy: 0.9916666666666667, F1 Score: 0.9915229885057472, Precision: 0.9916666666666667, Recall: 0.9919354838709677\n",
            "Epoch 12: Loss = 0.014370049771969207\n",
            "Accuracy: 0.9916666666666667, F1 Score: 0.9916749397549469, Precision: 0.9919642857142857, Recall: 0.9915034562211982\n",
            "Epoch 13: Loss = 0.0046706832508789375\n",
            "Accuracy: 1.0, F1 Score: 1.0, Precision: 1.0, Recall: 1.0\n",
            "Epoch 14: Loss = 0.010313720997146447\n",
            "Accuracy: 0.9958333333333333, F1 Score: 0.996150201374082, Precision: 0.99609375, Recall: 0.9963235294117647\n",
            "Epoch 15: Loss = 0.022760591440601274\n",
            "Accuracy: 0.9916666666666667, F1 Score: 0.9917532605569519, Precision: 0.9921182266009853, Recall: 0.9916573971078977\n",
            "Epoch 16: Loss = 0.0220777154318057\n",
            "Accuracy: 0.9875, F1 Score: 0.9867737346510932, Precision: 0.9880292338709677, Recall: 0.9866071428571428\n",
            "Epoch 17: Loss = 0.08063796476926655\n",
            "Accuracy: 0.9833333333333333, F1 Score: 0.983654912974135, Precision: 0.9840200029855202, Recall: 0.9836833193688033\n",
            "Epoch 18: Loss = 0.17983666094369255\n",
            "Accuracy: 0.9583333333333334, F1 Score: 0.9595762436876567, Precision: 0.9628143428853368, Recall: 0.9603919153868444\n",
            "Epoch 19: Loss = 0.19691360997967422\n",
            "Accuracy: 0.9333333333333333, F1 Score: 0.9330200191318988, Precision: 0.9382565065982404, Recall: 0.9339848241405527\n",
            "Epoch 20: Loss = 0.20962116960436106\n",
            "Accuracy: 0.9291666666666667, F1 Score: 0.9307929730502508, Precision: 0.9323025764558023, Recall: 0.9310738642205796\n",
            "Epoch 21: Loss = 0.1467076102271676\n",
            "Accuracy: 0.9375, F1 Score: 0.9366833690729689, Precision: 0.9388957863000201, Recall: 0.9362337012422075\n",
            "Epoch 22: Loss = 0.3818442225456238\n",
            "Accuracy: 0.8833333333333333, F1 Score: 0.8815195595715618, Precision: 0.8857059714902176, Recall: 0.882296166014186\n",
            "Epoch 23: Loss = 0.12539758358616382\n",
            "Accuracy: 0.9708333333333333, F1 Score: 0.9704287015152011, Precision: 0.9716965158497417, Recall: 0.9706646416839115\n",
            "Epoch 24: Loss = 0.2095492614316754\n",
            "Accuracy: 0.95, F1 Score: 0.9509014280791375, Precision: 0.9550239989895162, Recall: 0.9491593627091403\n",
            "Epoch 25: Loss = 0.09745118429418653\n",
            "Accuracy: 0.9833333333333333, F1 Score: 0.9833324258267546, Precision: 0.9831524527914615, Recall: 0.9839149609687136\n",
            "Epoch 26: Loss = 0.047537985403323546\n",
            "Accuracy: 0.9833333333333333, F1 Score: 0.9829756370009715, Precision: 0.9840923466160658, Recall: 0.9825634817639823\n",
            "Epoch 27: Loss = 0.07805978303440497\n",
            "Accuracy: 0.975, F1 Score: 0.974963052711465, Precision: 0.9762724438434378, Recall: 0.974694104560623\n",
            "Epoch 28: Loss = 0.009088629914913327\n",
            "Accuracy: 1.0, F1 Score: 1.0, Precision: 1.0, Recall: 1.0\n",
            "Epoch 29: Loss = 0.05015227055264404\n",
            "Accuracy: 0.9791666666666666, F1 Score: 0.9795163828597824, Precision: 0.98046875, Recall: 0.9795620834540713\n",
            "Epoch 30: Loss = 0.1296820861280139\n",
            "Accuracy: 0.9875, F1 Score: 0.9872195111027384, Precision: 0.9879515599343186, Recall: 0.9870689655172413\n",
            "Epoch 31: Loss = 0.04700767877875478\n",
            "Accuracy: 0.9833333333333333, F1 Score: 0.9826396531905006, Precision: 0.9838625672043011, Recall: 0.9826046798029557\n",
            "Epoch 32: Loss = 0.0656345414390671\n",
            "Accuracy: 0.9875, F1 Score: 0.9872808257217379, Precision: 0.9876251390433816, Recall: 0.9873470522803115\n",
            "Epoch 33: Loss = 0.11344538177945651\n",
            "Accuracy: 0.9791666666666666, F1 Score: 0.9786878485311488, Precision: 0.9815476190476191, Recall: 0.9781289910600255\n",
            "Epoch 34: Loss = 0.07585665088845417\n",
            "Accuracy: 0.9833333333333333, F1 Score: 0.9836705816920762, Precision: 0.9836705816920762, Recall: 0.9836705816920762\n",
            "Epoch 35: Loss = 0.02087812643731013\n",
            "Accuracy: 0.9958333333333333, F1 Score: 0.9959413459020685, Precision: 0.9963235294117647, Recall: 0.9956896551724138\n",
            "Epoch 36: Loss = 0.030602412269217893\n",
            "Accuracy: 0.9791666666666666, F1 Score: 0.9798334634400208, Precision: 0.9816176470588236, Recall: 0.9810606060606061\n",
            "Epoch 37: Loss = 0.03487606465932913\n",
            "Accuracy: 0.9833333333333333, F1 Score: 0.9835165778803544, Precision: 0.9854166666666666, Recall: 0.9826046798029557\n",
            "Epoch 38: Loss = 0.055975523246161174\n",
            "Accuracy: 0.9875, F1 Score: 0.9880730277185501, Precision: 0.9881827731092437, Recall: 0.9880713649096002\n",
            "Epoch 39: Loss = 0.11884395955712534\n",
            "Accuracy: 0.9666666666666667, F1 Score: 0.966212041637821, Precision: 0.9683035714285715, Recall: 0.9681295715778475\n",
            "Epoch 40: Loss = 0.159943346894579\n",
            "Accuracy: 0.9625, F1 Score: 0.9610459283914662, Precision: 0.9651470865025853, Recall: 0.959978997694515\n",
            "Evaluation on Test Dataset:\n",
            "Accuracy = 0.825, F1 Score = 0.8176671798448272, Precision = 0.8741031027795734, Recall = 0.8187810879500541\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Tentukan hyperparameter pelatihan\n",
        "learning_rate = 0.001\n",
        "num_epochs = 40\n",
        "batch_size = 32\n",
        "num_classes = len(label_encoder.classes_)  # Ganti dengan jumlah kelas yang sesuai\n",
        "\n",
        "# Tentukan model dan fungsi loss\n",
        "model = ResNeXt(num_classes)  # Ganti dengan model yang telah kamu bangun\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Buat DataLoader untuk memuat data pelatihan dalam batch\n",
        "train_dataset = torch.utils.data.TensorDataset(train_images, train_labels)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Melakukan pelatihan model\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for images, labels in train_loader:\n",
        "        # Reset gradien optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass dan optimasi\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Evaluasi prediksi\n",
        "        predicted_labels = torch.argmax(outputs, dim=1)\n",
        "        y_true.extend(labels.tolist())\n",
        "        y_pred.extend(predicted_labels.tolist())\n",
        "\n",
        "    # Cetak loss pada setiap epoch\n",
        "    print(f\"Epoch {epoch+1}: Loss = {running_loss / len(train_loader)}\")\n",
        "\n",
        "    # Hitung dan cetak metrik evaluasi\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    print(f\"Accuracy: {accuracy}, F1 Score: {f1}, Precision: {precision}, Recall: {recall}\")\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(test_images, test_labels)  # Ganti dengan dataset pengujian Anda\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Evaluasi model pada dataset pengujian\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "test_labels_list = []\n",
        "\n",
        "# Loop melalui setiap batch dari DataLoader pengujian\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Catat prediksi dan label untuk perhitungan metrik\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_predictions.extend(predicted.tolist())\n",
        "        test_labels_list.extend(labels.tolist())\n",
        "\n",
        "# Menghitung metrik evaluasi pada dataset pengujian\n",
        "test_accuracy = accuracy_score(test_labels_list, test_predictions)\n",
        "test_f1_score = f1_score(test_labels_list, test_predictions, average='macro')\n",
        "test_precision = precision_score(test_labels_list, test_predictions, average='macro')\n",
        "test_recall = recall_score(test_labels_list, test_predictions, average='macro')\n",
        "\n",
        "# Cetak hasil evaluasi pada dataset pengujian\n",
        "print(\"Evaluation on Test Dataset:\")\n",
        "print(f\"Accuracy = {test_accuracy}, F1 Score = {test_f1_score}, Precision = {test_precision}, Recall = {test_recall}\")\n",
        "\n",
        "# Setelah pelatihan selesai, model siap untuk digunakan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMfihxJiLgVP"
      },
      "outputs": [],
      "source": [
        "#torch.save(model.state_dict(), 'ResNeXt.pt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3FlLAo-myUa"
      },
      "source": [
        "# **ResNexT Trial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTczxINmm5OW",
        "outputId": "3dae89a7-adbd-4261-fbce-60e6bca5987d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNeXt(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): ResNeXtBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResNeXtBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): ResNeXtBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): ResNeXtBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResNeXtBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): ResNeXtBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): ResNeXtBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): ResNeXtBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResNeXtBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): ResNeXtBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): ResNeXtBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (4): ResNeXtBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (5): ResNeXtBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (6): ResNeXtBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): ResNeXtBlock(\n",
            "      (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResNeXtBlock(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): ResNeXtBlock(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): ResNeXtBlock(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (se): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=1024, out_features=8, bias=True)\n",
            ")\n",
            "Epoch 1: Loss = 1.75539131462574\n",
            "Accuracy: 0.3958333333333333, F1 Score: 0.3837611359531615, Precision: 0.4030489417989418, Recall: 0.38902061383535896\n",
            "Epoch 2: Loss = 0.4805318620055914\n",
            "Accuracy: 0.8541666666666666, F1 Score: 0.854706241092893, Precision: 0.8610327635327636, Recall: 0.8547370413598412\n",
            "Epoch 3: Loss = 0.2311572413891554\n",
            "Accuracy: 0.9333333333333333, F1 Score: 0.9336270685669641, Precision: 0.9376565920908593, Recall: 0.9333524104409239\n",
            "Epoch 4: Loss = 0.2355187755310908\n",
            "Accuracy: 0.9375, F1 Score: 0.9360433165694992, Precision: 0.9424428104575164, Recall: 0.9354661143454246\n",
            "Epoch 5: Loss = 0.3993274765089154\n",
            "Accuracy: 0.9291666666666667, F1 Score: 0.9292616175078158, Precision: 0.9304150132275132, Recall: 0.9306290988940829\n",
            "Epoch 6: Loss = 0.3207247434183955\n",
            "Accuracy: 0.8958333333333334, F1 Score: 0.892568921860526, Precision: 0.8942025812302428, Recall: 0.8965929005392788\n",
            "Epoch 7: Loss = 0.10831817309372127\n",
            "Accuracy: 0.9708333333333333, F1 Score: 0.9716920737763477, Precision: 0.9734670154024994, Recall: 0.9714286918124515\n",
            "Epoch 8: Loss = 0.07268567144637927\n",
            "Accuracy: 0.9875, F1 Score: 0.9879763576739224, Precision: 0.9881069345841785, Recall: 0.9883367139959433\n",
            "Epoch 9: Loss = 0.07608571613673121\n",
            "Accuracy: 0.9666666666666667, F1 Score: 0.9663736647808745, Precision: 0.9683028780011538, Recall: 0.9655589911777342\n",
            "Epoch 10: Loss = 0.10995039145927876\n",
            "Accuracy: 0.9875, F1 Score: 0.9869670911866919, Precision: 0.9873191194581281, Recall: 0.9869150246305419\n",
            "Epoch 11: Loss = 0.03568580379942432\n",
            "Accuracy: 0.9916666666666667, F1 Score: 0.9916937766898162, Precision: 0.9921568627450981, Recall: 0.9915034562211982\n",
            "Epoch 12: Loss = 0.07197632036695722\n",
            "Accuracy: 0.975, F1 Score: 0.9750420083444649, Precision: 0.9759199134199135, Recall: 0.9755088555798495\n",
            "Epoch 13: Loss = 0.08272336848312989\n",
            "Accuracy: 0.9708333333333333, F1 Score: 0.9701280462897398, Precision: 0.9732500527092558, Recall: 0.9699517320832671\n",
            "Epoch 14: Loss = 0.0678166156867519\n",
            "Accuracy: 0.9833333333333333, F1 Score: 0.9832422759661565, Precision: 0.9833996042413381, Recall: 0.9836070791953145\n",
            "Epoch 15: Loss = 0.022179610168677755\n",
            "Accuracy: 0.9875, F1 Score: 0.9881446876536887, Precision: 0.9888888888888889, Recall: 0.9878695183200189\n",
            "Epoch 16: Loss = 0.14781163213774562\n",
            "Accuracy: 0.9708333333333333, F1 Score: 0.9712876761968317, Precision: 0.9724753694581281, Recall: 0.9708650277337708\n",
            "Epoch 17: Loss = 0.0887187686166726\n",
            "Accuracy: 0.9791666666666666, F1 Score: 0.9794146764757027, Precision: 0.9801182569802115, Recall: 0.9798050021909867\n",
            "Epoch 18: Loss = 0.0727490033605136\n",
            "Accuracy: 0.9833333333333333, F1 Score: 0.9824116956864237, Precision: 0.9840923466160658, Recall: 0.9818121693121693\n",
            "Epoch 19: Loss = 0.07802313845604658\n",
            "Accuracy: 0.9791666666666666, F1 Score: 0.979671756070454, Precision: 0.9807539934077079, Recall: 0.9791739231394403\n",
            "Epoch 20: Loss = 0.07753599993884563\n",
            "Accuracy: 0.9666666666666667, F1 Score: 0.9661015574678512, Precision: 0.9673294401186504, Recall: 0.9676408130245727\n",
            "Epoch 21: Loss = 0.1677456504839938\n",
            "Accuracy: 0.9458333333333333, F1 Score: 0.9441572945740901, Precision: 0.9504255174291939, Recall: 0.9429731724601842\n",
            "Epoch 22: Loss = 0.08627219102345407\n",
            "Accuracy: 0.9833333333333333, F1 Score: 0.9837545980095156, Precision: 0.9845785440613026, Recall: 0.9834052326057332\n",
            "Epoch 23: Loss = 0.17410802911035717\n",
            "Accuracy: 0.9625, F1 Score: 0.9616493916375102, Precision: 0.9638607173889432, Recall: 0.9621553602283826\n",
            "Epoch 24: Loss = 0.09657214232720435\n",
            "Accuracy: 0.975, F1 Score: 0.9739056509220156, Precision: 0.9743778222495896, Recall: 0.9740332304987478\n",
            "Epoch 25: Loss = 0.04575315747933928\n",
            "Accuracy: 0.9833333333333333, F1 Score: 0.982915296608704, Precision: 0.983736559139785, Recall: 0.9830732101269627\n",
            "Epoch 26: Loss = 0.009555224249197636\n",
            "Accuracy: 1.0, F1 Score: 1.0, Precision: 1.0, Recall: 1.0\n",
            "Epoch 27: Loss = 0.006994294844844262\n",
            "Accuracy: 0.9958333333333333, F1 Score: 0.9956086286594761, Precision: 0.9958333333333333, Recall: 0.9955357142857143\n",
            "Epoch 28: Loss = 0.0014670605341962073\n",
            "Accuracy: 1.0, F1 Score: 1.0, Precision: 1.0, Recall: 1.0\n",
            "Epoch 29: Loss = 0.0015621168904544902\n",
            "Accuracy: 1.0, F1 Score: 1.0, Precision: 1.0, Recall: 1.0\n",
            "Epoch 30: Loss = 0.0005356932697395678\n",
            "Accuracy: 1.0, F1 Score: 1.0, Precision: 1.0, Recall: 1.0\n",
            "Epoch 31: Loss = 0.018201335396952345\n",
            "Accuracy: 0.9958333333333333, F1 Score: 0.9956140350877193, Precision: 0.9956896551724138, Recall: 0.9956896551724138\n",
            "Epoch 32: Loss = 0.025576645144610666\n",
            "Accuracy: 0.9958333333333333, F1 Score: 0.9956086286594761, Precision: 0.9958333333333333, Recall: 0.9955357142857143\n",
            "Epoch 33: Loss = 0.03392446735233534\n",
            "Accuracy: 0.9916666666666667, F1 Score: 0.9919090878375525, Precision: 0.9916573971078977, Recall: 0.9922912713472486\n",
            "Epoch 34: Loss = 0.07464735030953307\n",
            "Accuracy: 0.9791666666666666, F1 Score: 0.9799489144316731, Precision: 0.9825317604355717, Recall: 0.9786965676148102\n",
            "Epoch 35: Loss = 0.08959517837502062\n",
            "Accuracy: 0.9625, F1 Score: 0.9629022323105596, Precision: 0.9648428809788654, Recall: 0.9637423935091278\n",
            "Epoch 36: Loss = 0.12370186171028763\n",
            "Accuracy: 0.9708333333333333, F1 Score: 0.9707989444438776, Precision: 0.9732500527092558, Recall: 0.9710640791355475\n",
            "Epoch 37: Loss = 0.09508704242762178\n",
            "Accuracy: 0.9708333333333333, F1 Score: 0.9713737774866666, Precision: 0.9723311546840958, Recall: 0.9717844792887325\n",
            "Epoch 38: Loss = 0.07416426826966926\n",
            "Accuracy: 0.9875, F1 Score: 0.9879830059720178, Precision: 0.9886904761904762, Recall: 0.9876251390433816\n",
            "Epoch 39: Loss = 0.0758403348736465\n",
            "Accuracy: 0.9833333333333333, F1 Score: 0.9836527578250838, Precision: 0.9838709677419355, Recall: 0.9838372602555028\n",
            "Epoch 40: Loss = 0.021884961606701836\n",
            "Accuracy: 0.9916666666666667, F1 Score: 0.9919090878375525, Precision: 0.9922912713472486, Recall: 0.9916573971078977\n",
            "Evaluation on Test Dataset:\n",
            "Accuracy = 0.8, F1 Score = 0.7912658600829094, Precision = 0.847093099426859, Recall = 0.8001316594351491\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "class ResNeXtBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, cardinality=32):\n",
        "        super(ResNeXtBlock, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.cardinality = cardinality\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(residual)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNeXt(nn.Module):\n",
        "    def __init__(self, num_classes, cardinality=32):\n",
        "        super(ResNeXt, self).__init__()\n",
        "        self.cardinality = cardinality\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            ResNeXtBlock(64, 128, stride=1, cardinality=self.cardinality),\n",
        "            *[ResNeXtBlock(128, 128, stride=1, cardinality=self.cardinality) for _ in range(1, 3)]\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ResNeXtBlock(128, 256, stride=2, cardinality=self.cardinality),\n",
        "            *[ResNeXtBlock(256, 256, stride=1, cardinality=self.cardinality) for _ in range(1, 4)]\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ResNeXtBlock(256, 512, stride=2, cardinality=self.cardinality),\n",
        "            *[ResNeXtBlock(512, 512, stride=1, cardinality=self.cardinality) for _ in range(1, 7)]\n",
        "        )\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            ResNeXtBlock(512, 1024, stride=2, cardinality=self.cardinality),\n",
        "            *[ResNeXtBlock(1024, 1024, stride=1, cardinality=self.cardinality) for _ in range(1, 4)]\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_shape = (1, 224, 224)  # Input shape of your grayscale images in PyTorch convention (channels, height, width)\n",
        "num_classes = 8 # Number of output classes\n",
        "model = ResNeXt(num_classes)\n",
        "print(model)\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Tentukan hyperparameter pelatihan\n",
        "learning_rate = 0.001\n",
        "num_epochs = 40\n",
        "batch_size = 32\n",
        "num_classes = len(label_encoder.classes_)  # Ganti dengan jumlah kelas yang sesuai\n",
        "\n",
        "# Tentukan model dan fungsi loss\n",
        "model = ResNeXt(num_classes)  # Ganti dengan model yang telah kamu bangun\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Buat DataLoader untuk memuat data pelatihan dalam batch\n",
        "train_dataset = torch.utils.data.TensorDataset(train_images, train_labels)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Melakukan pelatihan model\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for images, labels in train_loader:\n",
        "        # Reset gradien optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass dan optimasi\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Evaluasi prediksi\n",
        "        predicted_labels = torch.argmax(outputs, dim=1)\n",
        "        y_true.extend(labels.tolist())\n",
        "        y_pred.extend(predicted_labels.tolist())\n",
        "\n",
        "    # Cetak loss pada setiap epoch\n",
        "    print(f\"Epoch {epoch+1}: Loss = {running_loss / len(train_loader)}\")\n",
        "\n",
        "    # Hitung dan cetak metrik evaluasi\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    print(f\"Accuracy: {accuracy}, F1 Score: {f1}, Precision: {precision}, Recall: {recall}\")\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(test_images, test_labels)  # Ganti dengan dataset pengujian Anda\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Evaluasi model pada dataset pengujian\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "test_labels_list = []\n",
        "\n",
        "# Loop melalui setiap batch dari DataLoader pengujian\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Catat prediksi dan label untuk perhitungan metrik\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_predictions.extend(predicted.tolist())\n",
        "        test_labels_list.extend(labels.tolist())\n",
        "\n",
        "# Menghitung metrik evaluasi pada dataset pengujian\n",
        "test_accuracy = accuracy_score(test_labels_list, test_predictions)\n",
        "test_f1_score = f1_score(test_labels_list, test_predictions, average='macro')\n",
        "test_precision = precision_score(test_labels_list, test_predictions, average='macro')\n",
        "test_recall = recall_score(test_labels_list, test_predictions, average='macro')\n",
        "# Cetak hasil evaluasi pada dataset pengujian\n",
        "print(\"Evaluation on Test Dataset:\")\n",
        "print(f\"Accuracy = {test_accuracy}, F1 Score = {test_f1_score}, Precision = {test_precision}, Recall = {test_recall}\")\n",
        "\n",
        "# Setelah pelatihan selesai, model siap untuk digunakan\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMH4McL18dFB"
      },
      "source": [
        "# **GhostNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4RX53Zx_gDd",
        "outputId": "d1d9ceb9-0eb4-4020-ed82-f509a4ad9e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GhostNet(\n",
            "  (stem): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (stage1): Sequential(\n",
            "    (0): GhostModule(\n",
            "      (primary_conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
            "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage2): Sequential(\n",
            "    (0): GhostModule(\n",
            "      (primary_conv): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=12, bias=False)\n",
            "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): GhostModule(\n",
            "      (primary_conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1), groups=12, bias=False)\n",
            "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage3): Sequential(\n",
            "    (0): GhostModule(\n",
            "      (primary_conv): Conv2d(24, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)\n",
            "        (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): GhostModule(\n",
            "      (primary_conv): Conv2d(40, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1), groups=20, bias=False)\n",
            "        (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage4): Sequential(\n",
            "    (0): GhostModule(\n",
            "      (primary_conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
            "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): GhostModule(\n",
            "      (primary_conv): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), groups=40, bias=False)\n",
            "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (2): GhostModule(\n",
            "      (primary_conv): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), groups=40, bias=False)\n",
            "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage5): Sequential(\n",
            "    (0): GhostModule(\n",
            "      (primary_conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
            "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): GhostModule(\n",
            "      (primary_conv): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), groups=80, bias=False)\n",
            "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (2): GhostModule(\n",
            "      (primary_conv): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), groups=80, bias=False)\n",
            "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage6): Sequential(\n",
            "    (0): GhostModule(\n",
            "      (primary_conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
            "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv7): Sequential(\n",
            "    (0): Conv2d(320, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (fc): Linear(in_features=1024, out_features=8, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Definisikan Ghost Module\n",
        "class GhostModule(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, kernel_size=1, ratio=2):\n",
        "        super(GhostModule, self).__init__()\n",
        "        internal_channels = int(output_channels / ratio)\n",
        "\n",
        "        self.primary_conv = nn.Conv2d(input_channels, internal_channels, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
        "        self.cheap_operation = nn.Sequential(\n",
        "            nn.Conv2d(internal_channels, internal_channels, kernel_size, stride=1, padding=kernel_size//2, groups=internal_channels, bias=False),\n",
        "            nn.BatchNorm2d(internal_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        primary_conv = self.primary_conv(x)\n",
        "        cheap_operation = self.cheap_operation(primary_conv)\n",
        "        return torch.cat((primary_conv, cheap_operation), 1)\n",
        "\n",
        "# Definisikan GhostNet\n",
        "class GhostNet(nn.Module):\n",
        "    def __init__(self, num_classes=8):\n",
        "        super(GhostNet, self).__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.stage1 = self._make_stage(16, 16, 1, 1)\n",
        "        self.stage2 = self._make_stage(16, 24, 2, 2)\n",
        "        self.stage3 = self._make_stage(24, 40, 2, 2)\n",
        "        self.stage4 = self._make_stage(40, 80, 3, 2)\n",
        "        self.stage5 = self._make_stage(80, 160, 3, 2)\n",
        "        self.stage6 = self._make_stage(160, 320, 1, 1)\n",
        "\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv2d(320, 1024, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def _make_stage(self, input_channels, output_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(GhostModule(input_channels, output_channels, kernel_size=3))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(GhostModule(output_channels, output_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stem(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.stage5(out)\n",
        "        out = self.stage6(out)\n",
        "        out = self.conv7(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Contoh penggunaan GhostNet untuk data grayscale\n",
        "input_shape = (1, 224, 224)  # Bentuk input gambar grayscale dalam konvensi PyTorch (channels, height, width)\n",
        "num_classes = 8  # Jumlah kelas output\n",
        "model = GhostNet(num_classes)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uofk7clQ8bem",
        "outputId": "d17655d7-dcd6-4cd2-8718-c3187f0b63b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss = 1.9557650089263916\n",
            "Accuracy: 0.25833333333333336\n",
            "F1 Score: 0.22836064447225396\n",
            "Precision: 0.2163897426498233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss = 1.4402070492506027\n",
            "Accuracy: 0.48333333333333334\n",
            "F1 Score: 0.4345353998549487\n",
            "Precision: 0.5160126289093087\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss = 1.1536832004785538\n",
            "Accuracy: 0.5708333333333333\n",
            "F1 Score: 0.5256973323623491\n",
            "Precision: 0.5245618386243387\n",
            "Epoch 4: Loss = 0.8775542825460434\n",
            "Accuracy: 0.725\n",
            "F1 Score: 0.6956980214824884\n",
            "Precision: 0.7738359311152057\n",
            "Epoch 5: Loss = 0.6632727310061455\n",
            "Accuracy: 0.8375\n",
            "F1 Score: 0.8300380698836076\n",
            "Precision: 0.8388540865585342\n",
            "Epoch 6: Loss = 0.5949083790183067\n",
            "Accuracy: 0.8208333333333333\n",
            "F1 Score: 0.8147193032651433\n",
            "Precision: 0.8348994702953871\n",
            "Epoch 7: Loss = 0.5671447217464447\n",
            "Accuracy: 0.8375\n",
            "F1 Score: 0.8331120042407859\n",
            "Precision: 0.8473421130653753\n",
            "Epoch 8: Loss = 0.46235090494155884\n",
            "Accuracy: 0.9208333333333333\n",
            "F1 Score: 0.9207656369841652\n",
            "Precision: 0.9225331791864051\n",
            "Epoch 9: Loss = 0.3081528767943382\n",
            "Accuracy: 0.9416666666666667\n",
            "F1 Score: 0.9409743320185523\n",
            "Precision: 0.9432271900695762\n",
            "Epoch 10: Loss = 0.36258222348988056\n",
            "Accuracy: 0.9125\n",
            "F1 Score: 0.9110137853461336\n",
            "Precision: 0.9112971184178776\n",
            "Epoch 11: Loss = 0.2250411920249462\n",
            "Accuracy: 0.9791666666666666\n",
            "F1 Score: 0.9787643618152093\n",
            "Precision: 0.978761574074074\n",
            "Epoch 12: Loss = 0.1422298699617386\n",
            "Accuracy: 0.9833333333333333\n",
            "F1 Score: 0.9828018947963801\n",
            "Precision: 0.9852813852813853\n",
            "Epoch 13: Loss = 0.1377912312746048\n",
            "Accuracy: 0.9708333333333333\n",
            "F1 Score: 0.9708609637763135\n",
            "Precision: 0.9715053763440861\n",
            "Epoch 14: Loss = 0.15863583702594042\n",
            "Accuracy: 0.9833333333333333\n",
            "F1 Score: 0.9837662337662338\n",
            "Precision: 0.9864864864864865\n",
            "Epoch 15: Loss = 0.08750726375728846\n",
            "Accuracy: 0.9916666666666667\n",
            "F1 Score: 0.9917014247522722\n",
            "Precision: 0.9919270833333333\n",
            "Epoch 16: Loss = 0.06255752826109529\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Epoch 17: Loss = 0.05659461813047528\n",
            "Accuracy: 0.9958333333333333\n",
            "F1 Score: 0.9958228905597326\n",
            "Precision: 0.99609375\n",
            "Epoch 18: Loss = 0.03492629318498075\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Epoch 19: Loss = 0.04400978295598179\n",
            "Accuracy: 0.9958333333333333\n",
            "F1 Score: 0.9955228653661656\n",
            "Precision: 0.9958333333333333\n",
            "Epoch 20: Loss = 0.04522885871119797\n",
            "Accuracy: 0.9958333333333333\n",
            "F1 Score: 0.9955342902711324\n",
            "Precision: 0.9955357142857143\n",
            "Evaluation on Test Dataset:\n",
            "Accuracy = 0.925, F1 Score = 0.9239234449760766, Precision = 0.9319652406417113\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
        "\n",
        "# Tentukan hyperparameter pelatihan\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "batch_size = 32\n",
        "num_classes = len(label_encoder.classes_)  # Ganti dengan jumlah kelas yang sesuai\n",
        "\n",
        "# Tentukan model dan fungsi loss\n",
        "model = GhostNet(num_classes)  # Ganti dengan model yang telah kamu bangun\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Buat DataLoader untuk memuat data pelatihan dalam batch\n",
        "train_dataset = torch.utils.data.TensorDataset(train_images, train_labels)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Melakukan pelatihan model\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    for images, labels in train_loader:\n",
        "        # Reset gradien optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass dan optimasi\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Prediksi dan target untuk perhitungan metrik\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.extend(predicted.tolist())\n",
        "        targets.extend(labels.tolist())\n",
        "\n",
        "    # Cetak loss pada setiap epoch\n",
        "    print(f\"Epoch {epoch+1}: Loss = {running_loss / len(train_loader)}\")\n",
        "\n",
        "    # Hitung dan cetak metrik evaluasi\n",
        "    accuracy = accuracy_score(targets, predictions)\n",
        "    f1 = f1_score(targets, predictions, average='macro')\n",
        "    precision = precision_score(targets, predictions, average='macro')\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "\n",
        "# Buat DataLoader untuk memuat data pengujian dalam batch\n",
        "test_dataset = torch.utils.data.TensorDataset(test_images, test_labels)  # Ganti dengan dataset pengujian Anda\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Evaluasi model pada dataset pengujian\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "test_labels_list = []\n",
        "\n",
        "# Loop melalui setiap batch dari DataLoader pengujian\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Catat prediksi dan label untuk perhitungan metrik\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_predictions.extend(predicted.tolist())\n",
        "        test_labels_list.extend(labels.tolist())\n",
        "\n",
        "# Menghitung metrik evaluasi pada dataset pengujian\n",
        "test_accuracy = accuracy_score(test_labels_list, test_predictions)\n",
        "test_f1_score = f1_score(test_labels_list, test_predictions, average='macro')\n",
        "test_precision = precision_score(test_labels_list, test_predictions, average='macro')\n",
        "\n",
        "# Cetak hasil evaluasi pada dataset pengujian\n",
        "print(\"Evaluation on Test Dataset:\")\n",
        "print(f\"Accuracy = {test_accuracy}, F1 Score = {test_f1_score}, Precision = {test_precision}\")\n",
        "# Setelah pelatihan selesai, model siap untuk digunakan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLZqGbEzPwm8"
      },
      "source": [
        "# **GhostNet Trial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9_41AI-LPmlf",
        "outputId": "fe8aade2-a45a-426d-bb15-c00c799b9c0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GhostNet(\n",
            "  (stem): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (stage1): Sequential(\n",
            "    (0): GhostModule(\n",
            "      (primary_conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
            "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (squeeze_excitation): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(8, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(2, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage2): Sequential(\n",
            "    (0): GhostModule(\n",
            "      (primary_conv): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=12, bias=False)\n",
            "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (squeeze_excitation): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "    (1): GhostModule(\n",
            "      (primary_conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1), groups=12, bias=False)\n",
            "        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (squeeze_excitation): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage3): Sequential(\n",
            "    (0): GhostModule(\n",
            "      (primary_conv): Conv2d(24, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)\n",
            "        (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (squeeze_excitation): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "    (1): GhostModule(\n",
            "      (primary_conv): Conv2d(40, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1), groups=20, bias=False)\n",
            "        (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (squeeze_excitation): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage4): Sequential(\n",
            "    (0): GhostModule(\n",
            "      (primary_conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
            "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (squeeze_excitation): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "    (1): GhostModule(\n",
            "      (primary_conv): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), groups=40, bias=False)\n",
            "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (squeeze_excitation): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "    (2): GhostModule(\n",
            "      (primary_conv): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), groups=40, bias=False)\n",
            "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (squeeze_excitation): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage5): Sequential(\n",
            "    (0): GhostModule(\n",
            "      (primary_conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
            "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (squeeze_excitation): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "    (1): GhostModule(\n",
            "      (primary_conv): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), groups=80, bias=False)\n",
            "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (squeeze_excitation): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "    (2): GhostModule(\n",
            "      (primary_conv): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), groups=80, bias=False)\n",
            "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (squeeze_excitation): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage6): Sequential(\n",
            "    (0): GhostModule(\n",
            "      (primary_conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (cheap_operation): Sequential(\n",
            "        (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
            "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (squeeze_excitation): SqueezeExcitation(\n",
            "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (fc2): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (sigmoid): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv7): Sequential(\n",
            "    (0): Conv2d(320, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (fc): Linear(in_features=1024, out_features=8, bias=True)\n",
            ")\n",
            "Epoch 1: Loss = 1.8759596645832062\n",
            "Accuracy: 0.2875\n",
            "F1 Score: 0.23418244536526672\n",
            "Precision: 0.22620470062474102\n",
            "Recall: 0.27964830919502776\n",
            "Epoch 2: Loss = 1.3378493040800095\n",
            "Accuracy: 0.4791666666666667\n",
            "F1 Score: 0.42875673096125544\n",
            "Precision: 0.4839532015688759\n",
            "Recall: 0.47096593578387075\n",
            "Epoch 3: Loss = 1.0088771656155586\n",
            "Accuracy: 0.7458333333333333\n",
            "F1 Score: 0.7436137978996715\n",
            "Precision: 0.7598799665371545\n",
            "Recall: 0.7434048674806379\n",
            "Epoch 4: Loss = 0.7986251078546047\n",
            "Accuracy: 0.7458333333333333\n",
            "F1 Score: 0.7215924902285769\n",
            "Precision: 0.7653456676085464\n",
            "Recall: 0.739839857219053\n",
            "Epoch 5: Loss = 0.5905459299683571\n",
            "Accuracy: 0.8375\n",
            "F1 Score: 0.826283476816192\n",
            "Precision: 0.8459694556569557\n",
            "Recall: 0.8330335273118921\n",
            "Epoch 6: Loss = 0.46750179678201675\n",
            "Accuracy: 0.8875\n",
            "F1 Score: 0.8833823459720767\n",
            "Precision: 0.899335407239819\n",
            "Recall: 0.8847794179412645\n",
            "Epoch 7: Loss = 0.33650979213416576\n",
            "Accuracy: 0.9458333333333333\n",
            "F1 Score: 0.9451375349278085\n",
            "Precision: 0.9463761838161294\n",
            "Recall: 0.9461369945895236\n",
            "Epoch 8: Loss = 0.26055932231247425\n",
            "Accuracy: 0.9625\n",
            "F1 Score: 0.9622251443960498\n",
            "Precision: 0.9670614919354839\n",
            "Recall: 0.9620045902373489\n",
            "Epoch 9: Loss = 0.21390489488840103\n",
            "Accuracy: 0.9708333333333333\n",
            "F1 Score: 0.9703878085710869\n",
            "Precision: 0.9709145021645023\n",
            "Recall: 0.9704330000840012\n",
            "Epoch 10: Loss = 0.13242741208523512\n",
            "Accuracy: 0.9833333333333333\n",
            "F1 Score: 0.9834357923497268\n",
            "Precision: 0.9838169642857143\n",
            "Recall: 0.984172077922078\n",
            "Epoch 11: Loss = 0.10256862500682473\n",
            "Accuracy: 0.9916666666666667\n",
            "F1 Score: 0.9914315192192088\n",
            "Precision: 0.9919270833333333\n",
            "Recall: 0.9912253694581281\n",
            "Epoch 12: Loss = 0.08727641450241208\n",
            "Accuracy: 0.9916666666666667\n",
            "F1 Score: 0.9916280402711324\n",
            "Precision: 0.9912253694581281\n",
            "Recall: 0.9924242424242424\n",
            "Epoch 13: Loss = 0.0799556509591639\n",
            "Accuracy: 0.9916666666666667\n",
            "F1 Score: 0.9917532605569519\n",
            "Precision: 0.9921182266009853\n",
            "Recall: 0.9916573971078977\n",
            "Epoch 14: Loss = 0.07864482887089252\n",
            "Accuracy: 0.9916666666666667\n",
            "F1 Score: 0.9920454545454546\n",
            "Precision: 0.9919354838709677\n",
            "Recall: 0.9926470588235294\n",
            "Epoch 15: Loss = 0.060002055717632174\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "Epoch 16: Loss = 0.037562692537903786\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "Epoch 17: Loss = 0.07961483299732208\n",
            "Accuracy: 0.9916666666666667\n",
            "F1 Score: 0.9920131845841785\n",
            "Precision: 0.9920131845841785\n",
            "Recall: 0.9920131845841785\n",
            "Epoch 18: Loss = 0.06291329488158226\n",
            "Accuracy: 0.9916666666666667\n",
            "F1 Score: 0.99120799198272\n",
            "Precision: 0.9921568627450981\n",
            "Recall: 0.9907407407407407\n",
            "Epoch 19: Loss = 0.06701035494916141\n",
            "Accuracy: 0.9916666666666667\n",
            "F1 Score: 0.9919367692430296\n",
            "Precision: 0.9927521008403362\n",
            "Recall: 0.9913793103448276\n",
            "Epoch 20: Loss = 0.0770624103024602\n",
            "Accuracy: 0.9791666666666666\n",
            "F1 Score: 0.9790260009476428\n",
            "Precision: 0.979812237394958\n",
            "Recall: 0.9786628601283773\n",
            "Epoch 21: Loss = 0.04009548807516694\n",
            "Accuracy: 0.9916666666666667\n",
            "F1 Score: 0.9915114257327372\n",
            "Precision: 0.9919354838709677\n",
            "Recall: 0.9915034562211982\n",
            "Epoch 22: Loss = 0.04121879651211202\n",
            "Accuracy: 0.9916666666666667\n",
            "F1 Score: 0.9916280402711324\n",
            "Precision: 0.9912253694581281\n",
            "Recall: 0.9924242424242424\n",
            "Epoch 23: Loss = 0.07093081041239202\n",
            "Accuracy: 0.9875\n",
            "F1 Score: 0.9875167903418678\n",
            "Precision: 0.9889508928571429\n",
            "Recall: 0.9867610837438423\n",
            "Epoch 24: Loss = 0.06235142063815147\n",
            "Accuracy: 0.9875\n",
            "F1 Score: 0.9873176954435462\n",
            "Precision: 0.987202380952381\n",
            "Recall: 0.9881138975966562\n",
            "Epoch 25: Loss = 0.0413215170847252\n",
            "Accuracy: 0.9916666666666667\n",
            "F1 Score: 0.9917735215064032\n",
            "Precision: 0.9915229885057472\n",
            "Recall: 0.9922912713472486\n",
            "Epoch 26: Loss = 0.02221350115723908\n",
            "Accuracy: 0.9958333333333333\n",
            "F1 Score: 0.9958228905597326\n",
            "Precision: 0.99609375\n",
            "Recall: 0.9956896551724138\n",
            "Epoch 27: Loss = 0.027201705321203917\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "Epoch 28: Loss = 0.015007122070528567\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "Epoch 29: Loss = 0.014554063556715846\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "Epoch 30: Loss = 0.006993512040935457\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "Epoch 31: Loss = 0.00477030711772386\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "Epoch 32: Loss = 0.013087237370200455\n",
            "Accuracy: 0.9958333333333333\n",
            "F1 Score: 0.9958321756043346\n",
            "Precision: 0.9958333333333333\n",
            "Recall: 0.9959677419354839\n",
            "Epoch 33: Loss = 0.011486437520943582\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "Epoch 34: Loss = 0.017351500020595267\n",
            "Accuracy: 0.9958333333333333\n",
            "F1 Score: 0.9958228905597326\n",
            "Precision: 0.99609375\n",
            "Recall: 0.9956896551724138\n",
            "Epoch 35: Loss = 0.0477898353128694\n",
            "Accuracy: 0.9875\n",
            "F1 Score: 0.9874643344078158\n",
            "Precision: 0.9877028397565923\n",
            "Recall: 0.9876251390433816\n",
            "Epoch 36: Loss = 0.05093031970318407\n",
            "Accuracy: 0.9875\n",
            "F1 Score: 0.9877775468170241\n",
            "Precision: 0.9879515599343186\n",
            "Recall: 0.9881138975966562\n",
            "Epoch 37: Loss = 0.03821993444580585\n",
            "Accuracy: 0.9958333333333333\n",
            "F1 Score: 0.9958616010854817\n",
            "Precision: 0.9963235294117647\n",
            "Recall: 0.9955357142857143\n",
            "Epoch 38: Loss = 0.019386579690035433\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "Epoch 39: Loss = 0.017126526567153633\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "Epoch 40: Loss = 0.008300976594910026\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "Evaluation on Test Dataset:\n",
            "Accuracy = 0.95625, F1 Score = 0.9538169197989481, Precision = 0.95625, Recall = 0.9588744588744589\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(self, input_channels, squeeze_ratio=4):\n",
        "        super(SqueezeExcitation, self).__init__()\n",
        "        squeeze_channels = input_channels // squeeze_ratio\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(input_channels, squeeze_channels, kernel_size=1, bias=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Conv2d(squeeze_channels, input_channels, kernel_size=1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.avg_pool(x)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        out = x * out\n",
        "        return out\n",
        "\n",
        "class GhostModule(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, kernel_size=1, ratio=2):\n",
        "        super(GhostModule, self).__init__()\n",
        "        internal_channels = int(output_channels / ratio)\n",
        "\n",
        "        self.primary_conv = nn.Conv2d(input_channels, internal_channels, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
        "        self.cheap_operation = nn.Sequential(\n",
        "            nn.Conv2d(internal_channels, internal_channels, kernel_size, stride=1, padding=kernel_size//2, groups=internal_channels, bias=False),\n",
        "            nn.BatchNorm2d(internal_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.squeeze_excitation = SqueezeExcitation(internal_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        primary_conv = self.primary_conv(x)\n",
        "        cheap_operation = self.cheap_operation(primary_conv)\n",
        "        cheap_operation = self.squeeze_excitation(cheap_operation)\n",
        "        return torch.cat((primary_conv, cheap_operation), 1)\n",
        "\n",
        "class GhostNet(nn.Module):\n",
        "    def __init__(self, num_classes=8):\n",
        "        super(GhostNet, self).__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.stage1 = self._make_stage(16, 16, 1, 1)\n",
        "        self.stage2 = self._make_stage(16, 24, 2, 2)\n",
        "        self.stage3 = self._make_stage(24, 40, 2, 2)\n",
        "        self.stage4 = self._make_stage(40, 80, 3, 2)\n",
        "        self.stage5 = self._make_stage(80, 160, 3, 2)\n",
        "        self.stage6 = self._make_stage(160, 320, 1, 1)\n",
        "\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv2d(320, 1024, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def _make_stage(self, input_channels, output_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(GhostModule(input_channels, output_channels, kernel_size=3))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(GhostModule(output_channels, output_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stem(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.stage5(out)\n",
        "        out = self.stage6(out)\n",
        "        out = self.conv7(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Contoh penggunaan GhostNet untuk data grayscale\n",
        "input_shape = (1, 224, 224)  # Bentuk input gambar grayscale dalam konvensi PyTorch (channels, height, width)\n",
        "num_classes = 8  # Jumlah kelas output\n",
        "model = GhostNet(num_classes)\n",
        "print(model)\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Tentukan hyperparameter pelatihan\n",
        "learning_rate = 0.001\n",
        "num_epochs = 40\n",
        "batch_size = 32\n",
        "num_classes = len(label_encoder.classes_)  # Ganti dengan jumlah kelas yang sesuai\n",
        "\n",
        "# Tentukan model dan fungsi loss\n",
        "model = GhostNet(num_classes)  # Ganti dengan model yang telah kamu bangun\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Buat DataLoader untuk memuat data pelatihan dalam batch\n",
        "train_dataset = torch.utils.data.TensorDataset(train_images, train_labels)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Melakukan pelatihan model\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    for images, labels in train_loader:\n",
        "        # Reset gradien optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass dan optimasi\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Prediksi dan target untuk perhitungan metrik\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.extend(predicted.tolist())\n",
        "        targets.extend(labels.tolist())\n",
        "\n",
        "    # Cetak loss pada setiap epoch\n",
        "    print(f\"Epoch {epoch+1}: Loss = {running_loss / len(train_loader)}\")\n",
        "\n",
        "    # Hitung dan cetak metrik evaluasi\n",
        "    accuracy = accuracy_score(targets, predictions)\n",
        "    f1 = f1_score(targets, predictions, average='macro')\n",
        "    precision = precision_score(targets, predictions, average='macro')\n",
        "    recall = recall_score(targets, predictions, average='macro')\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "# Buat DataLoader untuk memuat data pengujian dalam batch\n",
        "test_dataset = torch.utils.data.TensorDataset(test_images, test_labels)  # Ganti dengan dataset pengujian Anda\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Evaluasi model pada dataset pengujian\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "test_labels_list = []\n",
        "\n",
        "# Loop melalui setiap batch dari DataLoader pengujian\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Catat prediksi dan label untuk perhitungan metrik\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_predictions.extend(predicted.tolist())\n",
        "        test_labels_list.extend(labels.tolist())\n",
        "\n",
        "# Menghitung metrik evaluasi pada dataset pengujian\n",
        "test_accuracy = accuracy_score(test_labels_list, test_predictions)\n",
        "test_f1_score = f1_score(test_labels_list, test_predictions, average='macro')\n",
        "test_precision = precision_score(test_labels_list, test_predictions, average='macro')\n",
        "test_recall = recall_score(test_labels_list, test_predictions, average='macro')\n",
        "\n",
        "# Cetak hasil evaluasi pada dataset pengujian\n",
        "print(\"Evaluation on Test Dataset:\")\n",
        "print(f\"Accuracy = {test_accuracy}, F1 Score = {test_f1_score}, Precision = {test_precision}, Recall = {test_recall}\")\n",
        "# Setelah pelAturannya, jangan menambahkan data latih ke data pengujian. Namun, jika Anda ingin menambahkan beberapa data latih ke data pengujian, pastikan untuk membagi data dengan cara yang sama seperti membagi data latih dan data pengujian awal. Dengan kata lain, pastikan bahwa perbandingan kelas di data pengujian tetap seimbang dan mewakili distribusi kelas secara keseluruhan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFhjIbWWVSXi"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'GhostNet.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP2Fs_bzgx2L"
      },
      "source": [
        "# **MobileNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pJ9_acjgxf0",
        "outputId": "382dbd10-daa0-459b-ef7a-cd17947d6aff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 8])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import mobilenet_v2\n",
        "\n",
        "# Membuat model MobileNet untuk data grayscale\n",
        "class MobileNetGrayscale(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MobileNetGrayscale, self).__init__()\n",
        "        self.model = mobilenet_v2(pretrained=False)\n",
        "        self.model.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Menyesuaikan jumlah kelas di layer akhir\n",
        "        self.model.classifier[1] = nn.Linear(1280, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Inisialisasi model MobileNetGrayscale\n",
        "num_classes = 8  # Ganti dengan jumlah kelas yang sesuai\n",
        "model = MobileNetGrayscale(num_classes)\n",
        "\n",
        "# Contoh penggunaan model\n",
        "input_size = (1, 224, 224)  # Ubah sesuai dengan ukuran input yang diinginkan\n",
        "input_data = torch.randn(1, *input_size)\n",
        "output = model(input_data)\n",
        "print(output.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OOMfmTB5K9e",
        "outputId": "017a6d14-295f-433f-e863-eb2ce8b805a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss = 1.9998739659786224, Accuracy = 0.19583333333333333, F1 Score = 0.19780602617594295, Precision = 0.21297457930573066\n",
            "Epoch 2: Loss = 1.5053802728652954, Accuracy = 0.4041666666666667, F1 Score = 0.3456580495195961, Precision = 0.40384500864739836\n",
            "Epoch 3: Loss = 0.7567560821771622, Accuracy = 0.7625, F1 Score = 0.7525451671029231, Precision = 0.7646211011627366\n",
            "Epoch 4: Loss = 0.4142452944070101, Accuracy = 0.8708333333333333, F1 Score = 0.8687142700984372, Precision = 0.888826939798669\n",
            "Epoch 5: Loss = 0.2952400129288435, Accuracy = 0.9, F1 Score = 0.9017683275835722, Precision = 0.905504470373422\n",
            "Epoch 6: Loss = 0.24913461972028017, Accuracy = 0.9291666666666667, F1 Score = 0.928612230113516, Precision = 0.930262622725858\n",
            "Epoch 7: Loss = 0.4185210457071662, Accuracy = 0.9041666666666667, F1 Score = 0.9031137732916855, Precision = 0.9117980399230399\n",
            "Epoch 8: Loss = 0.5376371704041958, Accuracy = 0.825, F1 Score = 0.8224436245622492, Precision = 0.8263123448596363\n",
            "Epoch 9: Loss = 0.4397544488310814, Accuracy = 0.8625, F1 Score = 0.8630487774873215, Precision = 0.8701602229040308\n",
            "Epoch 10: Loss = 0.2613210380077362, Accuracy = 0.9125, F1 Score = 0.9116490438969131, Precision = 0.9142006290626447\n",
            "Epoch 11: Loss = 0.18032168690115213, Accuracy = 0.925, F1 Score = 0.9249997140931685, Precision = 0.9307780764635603\n",
            "Epoch 12: Loss = 0.12769646616652608, Accuracy = 0.9666666666666667, F1 Score = 0.9667481833915033, Precision = 0.9672854699064377\n",
            "Epoch 13: Loss = 0.13941731117665768, Accuracy = 0.95, F1 Score = 0.9484127853954392, Precision = 0.9508239537888494\n",
            "Epoch 14: Loss = 0.1786717341747135, Accuracy = 0.9375, F1 Score = 0.9367756416763022, Precision = 0.9395660681113267\n",
            "Epoch 15: Loss = 0.2276649591512978, Accuracy = 0.95, F1 Score = 0.9512725789590448, Precision = 0.9565357676645976\n",
            "Epoch 16: Loss = 0.16666246438398957, Accuracy = 0.9541666666666667, F1 Score = 0.9539524418991306, Precision = 0.9560786435786436\n",
            "Epoch 17: Loss = 0.10358085879124701, Accuracy = 0.9625, F1 Score = 0.96246838158362, Precision = 0.9639799613841953\n",
            "Epoch 18: Loss = 0.15247023152187467, Accuracy = 0.9666666666666667, F1 Score = 0.9666895046460704, Precision = 0.9681819950202304\n",
            "Epoch 19: Loss = 0.13137742830440402, Accuracy = 0.95, F1 Score = 0.9494778780671183, Precision = 0.9508042023817314\n",
            "Epoch 20: Loss = 0.0584895204519853, Accuracy = 0.9875, F1 Score = 0.986887346370105, Precision = 0.9871537755427842\n",
            "Evaluation on Test Dataset:\n",
            "Accuracy = 0.80625, F1 Score = 0.8017849442799012, Precision = 0.8096306861652129\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
        "\n",
        "# Definisikan hyperparameter pelatihan\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "batch_size = 32\n",
        "num_classes = 8  # Ganti dengan jumlah kelas yang sesuai\n",
        "\n",
        "# Inisialisasi model MobileNet\n",
        "model = MobileNetGrayscale(num_classes)\n",
        "\n",
        "# Definisikan fungsi loss dan optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Buat DataLoader untuk memuat data pelatihan dan pengujian dalam batch\n",
        "train_dataset = torch.utils.data.TensorDataset(train_images, train_labels)  # Ganti dengan dataset pelatihan Anda\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Melakukan pelatihan model\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    epoch_predictions = []\n",
        "    epoch_labels = []\n",
        "\n",
        "    # Loop melalui setiap batch dari DataLoader pelatihan\n",
        "    for images, labels in train_loader:\n",
        "        # Reset gradien optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass dan optimasi\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Catat prediksi dan label untuk perhitungan metrik\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        epoch_predictions.extend(predicted.tolist())\n",
        "        epoch_labels.extend(labels.tolist())\n",
        "\n",
        "    # Cetak loss, akurasi, f1 score, recall, precision, dan confusion matrix pada setiap epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = accuracy_score(epoch_labels, epoch_predictions)\n",
        "    epoch_f1_score = f1_score(epoch_labels, epoch_predictions, average='macro')\n",
        "    epoch_precision = precision_score(epoch_labels, epoch_predictions, average='macro')\n",
        "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss}, Accuracy = {epoch_accuracy}, F1 Score = {epoch_f1_score}, Precision = {epoch_precision}\")\n",
        "\n",
        "# Buat DataLoader untuk memuat data pengujian dalam batch\n",
        "test_dataset = torch.utils.data.TensorDataset(test_images, test_labels)  # Ganti dengan dataset pengujian Anda\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Evaluasi model pada dataset pengujian\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "test_labels_list = []\n",
        "\n",
        "# Loop melalui setiap batch dari DataLoader pengujian\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Catat prediksi dan label untuk perhitungan metrik\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_predictions.extend(predicted.tolist())\n",
        "        test_labels_list.extend(labels.tolist())\n",
        "\n",
        "# Menghitung metrik evaluasi pada dataset pengujian\n",
        "test_accuracy = accuracy_score(test_labels_list, test_predictions)\n",
        "test_f1_score = f1_score(test_labels_list, test_predictions, average='macro')\n",
        "test_precision = precision_score(test_labels_list, test_predictions, average='macro')\n",
        "\n",
        "# Cetak hasil evaluasi pada dataset pengujian\n",
        "print(\"Evaluation on Test Dataset:\")\n",
        "print(f\"Accuracy = {test_accuracy}, F1 Score = {test_f1_score}, Precision = {test_precision}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiSBMGCQs7Ty"
      },
      "source": [
        "# **MobileNet Trial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h26yuiEQtVpM",
        "outputId": "d3936337-777e-42bb-aa12-7ac86a9eb7e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 8])\n",
            "Epoch 1: Loss = 2.0778775066137314, Accuracy = 0.20416666666666666, F1 Score = 0.18531416483191432, Precision = 0.20256205576856243, Recall = 0.19653974259786272\n",
            "Epoch 2: Loss = 1.4290448427200317, Accuracy = 0.5291666666666667, F1 Score = 0.5060514019112632, Precision = 0.5684118825902713, Recall = 0.5188212259488351\n",
            "Epoch 3: Loss = 0.786224901676178, Accuracy = 0.7416666666666667, F1 Score = 0.7388861310112973, Precision = 0.7556993069747142, Recall = 0.7377310748908276\n",
            "Epoch 4: Loss = 0.40524337999522686, Accuracy = 0.8666666666666667, F1 Score = 0.868449567545243, Precision = 0.8738470175868756, Recall = 0.8694103846581271\n",
            "Epoch 5: Loss = 0.18575485050678253, Accuracy = 0.9333333333333333, F1 Score = 0.9337812988207557, Precision = 0.9385763422401354, Recall = 0.9313400571743176\n",
            "Epoch 6: Loss = 0.1812793263234198, Accuracy = 0.9625, F1 Score = 0.9620830196922012, Precision = 0.9621352364252462, Recall = 0.963133994623231\n",
            "Epoch 7: Loss = 0.16981642320752144, Accuracy = 0.9375, F1 Score = 0.9361304011995549, Precision = 0.9377139401289498, Recall = 0.9369876829901858\n",
            "Epoch 8: Loss = 0.39103016909211874, Accuracy = 0.8875, F1 Score = 0.8863898356545415, Precision = 0.8943393361132331, Recall = 0.8851481191458617\n",
            "Epoch 9: Loss = 0.30805573612451553, Accuracy = 0.9083333333333333, F1 Score = 0.9079902441207109, Precision = 0.9107257617659801, Recall = 0.9080369482204855\n",
            "Epoch 10: Loss = 0.28721165377646685, Accuracy = 0.9041666666666667, F1 Score = 0.9015783069641232, Precision = 0.9032665804718085, Recall = 0.9024637263305227\n",
            "Epoch 11: Loss = 0.24978027446195483, Accuracy = 0.9416666666666667, F1 Score = 0.9413705652564176, Precision = 0.9457384990858039, Recall = 0.9398238632763922\n",
            "Epoch 12: Loss = 0.23164575593546033, Accuracy = 0.9291666666666667, F1 Score = 0.9304658595328784, Precision = 0.9324860699130721, Recall = 0.9311305951240846\n",
            "Epoch 13: Loss = 0.1785546327009797, Accuracy = 0.9416666666666667, F1 Score = 0.9412256528781953, Precision = 0.9428674055829228, Recall = 0.9417077510097532\n",
            "Epoch 14: Loss = 0.22195610962808132, Accuracy = 0.9375, F1 Score = 0.9367528995220645, Precision = 0.9400978812521152, Recall = 0.9355951316381206\n",
            "Epoch 15: Loss = 0.19846775382757187, Accuracy = 0.9416666666666667, F1 Score = 0.9424135564257119, Precision = 0.9451772223767149, Recall = 0.9429843246890783\n",
            "Epoch 16: Loss = 0.105883052572608, Accuracy = 0.9625, F1 Score = 0.9627010792196686, Precision = 0.9626026005058264, Recall = 0.9638043715368193\n",
            "Epoch 17: Loss = 0.09155916224699467, Accuracy = 0.9791666666666666, F1 Score = 0.9787580826751149, Precision = 0.9810096765979119, Recall = 0.9779864532019704\n",
            "Epoch 18: Loss = 0.0628952207043767, Accuracy = 0.9791666666666666, F1 Score = 0.9789982967191535, Precision = 0.979667125470878, Recall = 0.978726362625139\n",
            "Epoch 19: Loss = 0.044885094568599015, Accuracy = 0.9916666666666667, F1 Score = 0.9916080133965057, Precision = 0.9921568627450981, Recall = 0.9913381123058542\n",
            "Epoch 20: Loss = 0.02525454806163907, Accuracy = 0.9958333333333333, F1 Score = 0.9955342902711324, Precision = 0.9955357142857143, Recall = 0.9956896551724138\n",
            "Epoch 21: Loss = 0.0883691729977727, Accuracy = 0.9875, F1 Score = 0.9872280669609486, Precision = 0.9876925770308124, Recall = 0.987193111393612\n",
            "Epoch 22: Loss = 0.10697315819561481, Accuracy = 0.9833333333333333, F1 Score = 0.9836829836829837, Precision = 0.9857142857142858, Recall = 0.9832810867293627\n",
            "Epoch 23: Loss = 0.0951451490400359, Accuracy = 0.9708333333333333, F1 Score = 0.9714893664069164, Precision = 0.9717765231092437, Recall = 0.9714638598386063\n",
            "Epoch 24: Loss = 0.13835606328211725, Accuracy = 0.9708333333333333, F1 Score = 0.9716435976304396, Precision = 0.9752316972144559, Recall = 0.9703837597330367\n",
            "Epoch 25: Loss = 0.09804750303737819, Accuracy = 0.9791666666666666, F1 Score = 0.9795043013130937, Precision = 0.9796266233766234, Recall = 0.9801959576934223\n",
            "Epoch 26: Loss = 0.20027649402618408, Accuracy = 0.9291666666666667, F1 Score = 0.9291111738302951, Precision = 0.9292276666490968, Recall = 0.9294922039556601\n",
            "Epoch 27: Loss = 0.1679042628966272, Accuracy = 0.9375, F1 Score = 0.9381242606161773, Precision = 0.942586442595517, Recall = 0.9369337472744854\n",
            "Epoch 28: Loss = 0.1035134457051754, Accuracy = 0.9833333333333333, F1 Score = 0.9835689244013043, Precision = 0.9839016317733991, Recall = 0.98380355276907\n",
            "Epoch 29: Loss = 0.14807281002867967, Accuracy = 0.9541666666666667, F1 Score = 0.9543973610798848, Precision = 0.9557801638698816, Recall = 0.9548643970796036\n",
            "Epoch 30: Loss = 0.12815218302421272, Accuracy = 0.9625, F1 Score = 0.9621410113304092, Precision = 0.9658310027120579, Recall = 0.9604143860609378\n",
            "Epoch 31: Loss = 0.08483210991835222, Accuracy = 0.9708333333333333, F1 Score = 0.9712387059643928, Precision = 0.9720094086021506, Recall = 0.9724183535280833\n",
            "Epoch 32: Loss = 0.15768073173239827, Accuracy = 0.9708333333333333, F1 Score = 0.9714721681826946, Precision = 0.9714065336545176, Recall = 0.9725297617277268\n",
            "Epoch 33: Loss = 0.11990152508951724, Accuracy = 0.9666666666666667, F1 Score = 0.9673400760910486, Precision = 0.9700748847926267, Recall = 0.9658020358062889\n",
            "Epoch 34: Loss = 0.10095060290768743, Accuracy = 0.9708333333333333, F1 Score = 0.9711865191845124, Precision = 0.9709154953315131, Recall = 0.9724183535280833\n",
            "Epoch 35: Loss = 0.1637706380279269, Accuracy = 0.9791666666666666, F1 Score = 0.9794332306314607, Precision = 0.9800595238095238, Recall = 0.9799164103906302\n",
            "Epoch 36: Loss = 0.038687649357598275, Accuracy = 0.9791666666666666, F1 Score = 0.9805864145183175, Precision = 0.9826388888888888, Recall = 0.9808298319327731\n",
            "Epoch 37: Loss = 0.06193031161092222, Accuracy = 0.9791666666666666, F1 Score = 0.9797314627348084, Precision = 0.9800502542507549, Recall = 0.9799941111038408\n",
            "Epoch 38: Loss = 0.14157882193103433, Accuracy = 0.9708333333333333, F1 Score = 0.9701768903831136, Precision = 0.9713847216002389, Recall = 0.9701663163495755\n",
            "Epoch 39: Loss = 0.09907542559085414, Accuracy = 0.9583333333333334, F1 Score = 0.957547076214003, Precision = 0.9607649071358749, Recall = 0.9568221979581825\n",
            "Epoch 40: Loss = 0.13466259744018316, Accuracy = 0.9583333333333334, F1 Score = 0.9581439863629361, Precision = 0.9605581045236218, Recall = 0.958411131415859\n",
            "Evaluation on Test Dataset:\n",
            "Accuracy = 0.88125, F1 Score = 0.8813649675357174, Precision = 0.8863235740867319, Recall = 0.8878217034681566\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import mobilenet_v2\n",
        "\n",
        "class MobileNetGrayscale(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MobileNetGrayscale, self).__init__()\n",
        "        self.model = mobilenet_v2(pretrained=False)\n",
        "\n",
        "        # Mengubah lapisan konvolusi pertama\n",
        "        self.model.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Menyesuaikan jumlah kelas di layer akhir\n",
        "        self.model.classifier[1] = nn.Linear(1280, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Inisialisasi model MobileNetGrayscale\n",
        "num_classes = 8\n",
        "model = MobileNetGrayscale(num_classes)\n",
        "\n",
        "# Contoh penggunaan model\n",
        "input_size = (1, 224, 224)\n",
        "input_data = torch.randn(1, *input_size)\n",
        "output = model(input_data)\n",
        "print(output.size())\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Definisikan hyperparameter pelatihan\n",
        "learning_rate = 0.001\n",
        "num_epochs = 40\n",
        "batch_size = 32\n",
        "num_classes = 8  # Ganti dengan jumlah kelas yang sesuai\n",
        "\n",
        "# Inisialisasi model MobileNet\n",
        "model = MobileNetGrayscale(num_classes)\n",
        "\n",
        "# Definisikan fungsi loss dan optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Buat DataLoader untuk memuat data pelatihan dan pengujian dalam batch\n",
        "train_dataset = torch.utils.data.TensorDataset(train_images, train_labels)  # Ganti dengan dataset pelatihan Anda\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Melakukan pelatihan model\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    epoch_predictions = []\n",
        "    epoch_labels = []\n",
        "\n",
        "    # Loop melalui setiap batch dari DataLoader pelatihan\n",
        "    for images, labels in train_loader:\n",
        "        # Reset gradien optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass dan optimasi\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Catat prediksi dan label untuk perhitungan metrik\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        epoch_predictions.extend(predicted.tolist())\n",
        "        epoch_labels.extend(labels.tolist())\n",
        "\n",
        "    # Cetak loss, akurasi, f1 score, recall, precision, dan confusion matrix pada setiap epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = accuracy_score(epoch_labels, epoch_predictions)\n",
        "    epoch_f1_score = f1_score(epoch_labels, epoch_predictions, average='macro')\n",
        "    epoch_precision = precision_score(epoch_labels, epoch_predictions, average='macro')\n",
        "    epoch_recall = recall_score(epoch_labels, epoch_predictions, average='macro')\n",
        "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss}, Accuracy = {epoch_accuracy}, F1 Score = {epoch_f1_score}, Precision = {epoch_precision}, Recall = {epoch_recall}\")\n",
        "\n",
        "# Buat DataLoader untuk memuat data pengujian dalam batch\n",
        "test_dataset = torch.utils.data.TensorDataset(test_images, test_labels)  # Ganti dengan dataset pengujian Anda\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Evaluasi model pada dataset pengujian\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "test_labels_list = []\n",
        "\n",
        "# Loop melalui setiap batch dari DataLoader pengujian\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Catat prediksi dan label untuk perhitungan metrik\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_predictions.extend(predicted.tolist())\n",
        "        test_labels_list.extend(labels.tolist())\n",
        "\n",
        "# Menghitung metrik evaluasi pada dataset pengujian\n",
        "test_accuracy = accuracy_score(test_labels_list, test_predictions)\n",
        "test_f1_score = f1_score(test_labels_list, test_predictions, average='macro')\n",
        "test_precision = precision_score(test_labels_list, test_predictions, average='macro')\n",
        "test_recall = recall_score(test_labels_list, test_predictions, average='macro')\n",
        "\n",
        "# Cetak hasil evaluasi pada dataset pengujian\n",
        "print(\"Evaluation on Test Dataset:\")\n",
        "print(f\"Accuracy = {test_accuracy}, F1 Score = {test_f1_score}, Precision = {test_precision}, Recall = {test_recall}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttKJQxQqtsLs",
        "outputId": "0a973585-ace6-4e35-db34-2b286b9cffb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss = 2.013596475124359, Accuracy = 0.22083333333333333, F1 Score = 0.21083397877405838, Precision = 0.3073209482418083, Recall = 0.2136279639101402\n",
            "Epoch 2: Loss = 1.3452610224485397, Accuracy = 0.5458333333333333, F1 Score = 0.508972769672869, Precision = 0.6267723420060627, Recall = 0.5334487230527766\n",
            "Epoch 3: Loss = 0.7639777436852455, Accuracy = 0.7833333333333333, F1 Score = 0.7828174027811697, Precision = 0.7933494792901487, Recall = 0.7803188700284329\n",
            "Epoch 4: Loss = 0.38657487742602825, Accuracy = 0.8958333333333334, F1 Score = 0.8958444982061446, Precision = 0.9035776499486177, Recall = 0.8939604109584643\n",
            "Epoch 5: Loss = 0.27663485426455736, Accuracy = 0.9041666666666667, F1 Score = 0.9037611592217747, Precision = 0.9043365396075005, Recall = 0.9054879818412012\n",
            "Epoch 6: Loss = 0.24881551042199135, Accuracy = 0.9125, F1 Score = 0.9112611542616319, Precision = 0.916877787718305, Recall = 0.9100313776649411\n",
            "Epoch 7: Loss = 0.27584611251950264, Accuracy = 0.8708333333333333, F1 Score = 0.8718927579933271, Precision = 0.8794888243476953, Recall = 0.8689900747661005\n",
            "Epoch 8: Loss = 0.3414067570120096, Accuracy = 0.9041666666666667, F1 Score = 0.9016710335657588, Precision = 0.9023842990528197, Recall = 0.9036046596366887\n",
            "Epoch 9: Loss = 0.252951730042696, Accuracy = 0.9083333333333333, F1 Score = 0.907237845596208, Precision = 0.907472233733439, Recall = 0.9081071583811556\n",
            "Epoch 10: Loss = 0.19783352478407323, Accuracy = 0.9416666666666667, F1 Score = 0.9407209024339998, Precision = 0.9423475195652615, Recall = 0.9405116732314436\n",
            "Epoch 11: Loss = 0.15741797722876072, Accuracy = 0.9583333333333334, F1 Score = 0.9581347195001626, Precision = 0.9598908185923273, Recall = 0.9580736719919145\n",
            "Epoch 12: Loss = 0.17827819054946303, Accuracy = 0.9458333333333333, F1 Score = 0.9464794318229623, Precision = 0.9481111202016375, Recall = 0.9456435406715129\n",
            "Epoch 13: Loss = 0.14160393131896853, Accuracy = 0.9416666666666667, F1 Score = 0.9423292380778574, Precision = 0.9461865794704979, Recall = 0.9418117945484955\n",
            "Epoch 14: Loss = 0.28202634351328015, Accuracy = 0.9125, F1 Score = 0.9129775270815781, Precision = 0.918648008717567, Recall = 0.9148334237628715\n",
            "Epoch 15: Loss = 0.16903263074345887, Accuracy = 0.9458333333333333, F1 Score = 0.9458712340291288, Precision = 0.9469596674876848, Recall = 0.9464502493787483\n",
            "Epoch 16: Loss = 0.2258018022403121, Accuracy = 0.925, F1 Score = 0.9238591007197003, Precision = 0.9319979329462088, Recall = 0.9215697349039953\n",
            "Epoch 17: Loss = 0.2905696742236614, Accuracy = 0.925, F1 Score = 0.9266510599680731, Precision = 0.9300753514923774, Recall = 0.9268805179682462\n",
            "Epoch 18: Loss = 0.07140282611362636, Accuracy = 0.9708333333333333, F1 Score = 0.9700674620265544, Precision = 0.970594875881387, Recall = 0.9705079056094074\n",
            "Epoch 19: Loss = 0.07931685051880777, Accuracy = 0.975, F1 Score = 0.9757645978145373, Precision = 0.9774298512508452, Recall = 0.9748547531112309\n",
            "Epoch 20: Loss = 0.13901716959662735, Accuracy = 0.9708333333333333, F1 Score = 0.9710776262229522, Precision = 0.9728083697911285, Recall = 0.9710953346855984\n",
            "Epoch 21: Loss = 0.10430160438409075, Accuracy = 0.9583333333333334, F1 Score = 0.9582594492387029, Precision = 0.9594002130766837, Recall = 0.9576626141518505\n",
            "Epoch 22: Loss = 0.09233598888386041, Accuracy = 0.9708333333333333, F1 Score = 0.9716202560471596, Precision = 0.9709399332591768, Recall = 0.9734632856074982\n",
            "Epoch 23: Loss = 0.05793110164813697, Accuracy = 0.9875, F1 Score = 0.9876449221276808, Precision = 0.987995526384535, Recall = 0.9874374906702493\n",
            "Epoch 24: Loss = 0.029739916790276766, Accuracy = 0.9916666666666667, F1 Score = 0.9912921274871231, Precision = 0.9912253694581281, Recall = 0.9916573971078977\n",
            "Epoch 25: Loss = 0.02794077107682824, Accuracy = 0.9916666666666667, F1 Score = 0.9917811695688592, Precision = 0.9919270833333333, Recall = 0.991901776384535\n",
            "Epoch 26: Loss = 0.0367922259029001, Accuracy = 0.9875, F1 Score = 0.9874678723569551, Precision = 0.9886904761904762, Recall = 0.9869150246305418\n",
            "Epoch 27: Loss = 0.015250777520122938, Accuracy = 0.9958333333333333, F1 Score = 0.9956573824498353, Precision = 0.99609375, Recall = 0.9953703703703703\n",
            "Epoch 28: Loss = 0.05645525426371023, Accuracy = 0.9875, F1 Score = 0.9871619827128302, Precision = 0.9877688172043011, Recall = 0.9872835497835498\n",
            "Epoch 29: Loss = 0.035291525346110575, Accuracy = 0.9875, F1 Score = 0.9879222577636686, Precision = 0.9887408088235294, Recall = 0.9873835549545489\n",
            "Epoch 30: Loss = 0.06562252528965473, Accuracy = 0.9916666666666667, F1 Score = 0.991418230891915, Precision = 0.9912253694581281, Recall = 0.991901776384535\n",
            "Epoch 31: Loss = 0.010221954507869668, Accuracy = 1.0, F1 Score = 1.0, Precision = 1.0, Recall = 1.0\n",
            "Epoch 32: Loss = 0.02980932590435259, Accuracy = 0.9958333333333333, F1 Score = 0.9957431457431458, Precision = 0.99609375, Recall = 0.9955357142857143\n",
            "Epoch 33: Loss = 0.02090857963776216, Accuracy = 0.9958333333333333, F1 Score = 0.9957578372159908, Precision = 0.9956896551724138, Recall = 0.9959677419354839\n",
            "Epoch 34: Loss = 0.043650328007061034, Accuracy = 0.9833333333333333, F1 Score = 0.9832276659252065, Precision = 0.9828920361247948, Recall = 0.9838372602555028\n",
            "Epoch 35: Loss = 0.010720815844251774, Accuracy = 0.9958333333333333, F1 Score = 0.9956883734760631, Precision = 0.9958333333333333, Recall = 0.9956896551724138\n",
            "Epoch 36: Loss = 0.03595189415500499, Accuracy = 0.9958333333333333, F1 Score = 0.9956883734760631, Precision = 0.9958333333333333, Recall = 0.9956896551724138\n",
            "Epoch 37: Loss = 0.018455399127560668, Accuracy = 0.9958333333333333, F1 Score = 0.9959582790091265, Precision = 0.9958333333333333, Recall = 0.9962121212121212\n",
            "Epoch 38: Loss = 0.009877303178654984, Accuracy = 1.0, F1 Score = 1.0, Precision = 1.0, Recall = 1.0\n",
            "Epoch 39: Loss = 0.03865817608311772, Accuracy = 0.9916666666666667, F1 Score = 0.991475636173201, Precision = 0.991859243697479, Recall = 0.9913793103448276\n",
            "Epoch 40: Loss = 0.06332567971548997, Accuracy = 0.975, F1 Score = 0.9728168680407486, Precision = 0.9748857226095207, Recall = 0.9725415070242657\n",
            "Evaluation on Test Dataset:\n",
            "Accuracy = 0.83125, F1 Score = 0.8297382339704227, Precision = 0.8402967721562923, Recall = 0.8350857316870997\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOpF2d0DVaql"
      },
      "outputs": [],
      "source": [
        "#torch.save(model.state_dict(), 'MobileNet.pt')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Z1IG4bL_7dg4",
        "o3FlLAo-myUa",
        "eMH4McL18dFB"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNFysYBnI6y5qpFgRn8LTvd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}